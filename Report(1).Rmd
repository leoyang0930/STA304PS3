---
title: ""
author: ""
date: 2020-10-04
bibliography: A3.bib
fontsize: 10pt
link-citations: yes
linkcolor: blue
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r survey data setup, include=FALSE}
library(haven)
library(tidyverse)
library(magrittr)
knitr::opts_chunk$set(echo = TRUE)
survey_data <- read_csv("survey_data.csv")
parse_number_full <- function(x) {
  purrr::map_dbl(str_extract_all(
    str_replace_all("$60,000 to $64,999", ",", ""), "\\d+"), ~mean(as.numeric(.x)))
}
survey_data %<>% 
  mutate(
    education = case_when(
      education %in% c("High school graduate", 
                       "Middle School - Grades 4 - 8", 
                       "Completed some high school",
                       "3rd Grade or less",
                       "Other post high school vocational training") ~ "highSchool", 
      education %in% c("Associate Degree", 
                       "College Degree (such as B.A., B.S.)", 
                       "Completed some college, but no degree") ~ "bachelor",
      education %in% c("Masters degree", 
                       "Completed some graduate, but no degree", 
                       "Doctorate degree") ~ "graduate"
    ),
    education = factor(education, levels = c("highSchool", "bachelor", "graduate")),
    hispanic = case_when(
      hispanic == "Not Hispanic" ~ "not hispanic", 
      hispanic == "Mexican" ~ "mexican",
      hispanic == "Puerto Rican" ~ "puerto rican",
      hispanic == "Cuban" ~ "cuban",
      TRUE ~ "other"
    ),
    citizen = case_when(
      foreign_born == "The United States" ~ "naturalized citizen",
      foreign_born == "Another country" ~ "not a citizen",
    ),
    sex = gender,
    race = case_when(
      race_ethnicity == "White" ~ "white",
      race_ethnicity == "Black, or African American" ~ "black",
      grepl("Asian", race_ethnicity) ~ "asian",
      race_ethnicity == "American Indian or Alaska Native" ~ "american indian or alaska native",
      grepl("Pacific", race_ethnicity) ~ "pacific",
      TRUE ~ "other"
    ),
    inctot = case_when(# empirical rule
      household_income == "Less than $14,999" ~ rnorm(n(), mean = 14999/2, sd = 5000/6),
      household_income == "$250,000 and above" ~ rnorm(n(), mean = 275000, sd = 50000/6),
      TRUE ~ rnorm(n(), mean = parse_number_full(household_income), sd = 5000/6) 
    )
  ) %>% 
  # census data need to modify: citizen (cut some levels); census_region (shorten levels);
  # race (merge levels)
  # inctot (remove negative)
  # state (refactor)
  # 
  select(
    citizen, sex, census_region, 
    hispanic, race, education, inctot, state,
    age, vote_trump
  ) %>%
  na.omit()
```

# 2020 Election Forecast, Trump or Biden?

## Zishan Cheng, Luluyang Yang, Shuyan Dong, Chengxuan Zhang
## 2020-10-31

# Model

We are interested in predicting the popular vote outcome of the 2020 American federal election. The survey data is collected by @data. To do this we are employing a post-stratification technique [@lohr2009sampling] [@Sampling]. In the following sub-sections we will describe the model specifics and the post-stratification calculation (some data modification details are explained in appendix).

## Model Specifics

A typical model model would be the generalized linear model where to accommodate the binary response. 
\[
  \begin{array}{lcl}  
  \mu &=& \beta_0 + \beta_1\text{citizen}+ \beta_2\text{sex} + \beta_3\text{region} + \beta_4\text{hispanic} + 
    \\&&\\
    && \beta_5\text{race} + \beta_6\text{education} + \beta_7\text{inctot}  + \beta_8\text{age} + \beta_9\text{state} + e
  \end{array} 
\]
where $\mu = log(\frac{p}{1-p})$; $p$ represents the chance to vote for Trump; $\beta$ represents the coefficients (the true number of coefficients would be slightly different from here due to the dummy variables); $e\sim N(0, \sigma^2)$ represents the residuals following. 

Since some covariates may highly correlated (multi-collinearity issues), to balance the number of variables and the goodness of fit, a stepwise regression respect to the BIC [@schwarz1978estimating] criteria (compared with AIC [@akaike1998information], BIC gives more penalty on the number of coefficients) would be performed. After the selection, our model is turned as
\[
  \begin{array}{lcl}  
  log(\frac{p}{1-p}) &=& \beta_0 + \beta_1\text{citizen}+ \beta_2\text{sex} + \beta_3\text{region} + 
    \\&&\\
    && \beta_4\text{race} + \beta_5\text{inctot}  + \beta_6\text{age} + e
  \end{array} 
\]
```{r models, fig.width=8, fig.height=4, echo=FALSE, message = FALSE, warning = FALSE, error=FALSE}
model <- MASS::stepAIC(glm(vote_trump~., data = survey_data, 
                           family = "binomial"), 
                       trace = FALSE,
                       k = log(dim(survey_data)[1]))
```

## Forecast

In order to estimate the proportion of voters who will vote for Donald Trump. We need to access the census data [@censusdata]. 

### Data Preparation

The census data literally contains all information to predict our model, however, due to the different collection methods or survey questionnaires, the information does not perfectly match the survey data. Hence, we have to clean the raw data to make the variable names and factor levels identical to the survey data.

```{r census data setup, include=FALSE}
census_data <- read_csv("census_data.csv") 
census_data %<>% 
  mutate(
    citizen = case_when(
      citizen %in% c("naturalized citizen", 
                     "born abroad of american parents") ~ "naturalized citizen",
      TRUE ~ "not a citizen"
    ),
    sex = case_when(
      sex == "female" ~ "Female", 
      sex == "male" ~ "Male"),
    census_region = case_when(
      region %in% c("east south central div", 
                    "west south central div",
                    "south atlantic division") ~ "South",
      region %in% c("east north central div",
                    "new england division") ~ "Northeast",
      region %in% c("pacific division",
                    "mountain division",
                    "west north central div") ~ "West",
      region %in% c("middle atlantic division") ~ "Midwest"
    ),
    race = case_when(
      race == "black/african american/negro" ~ "black",
      race %in% c("chinese", "japanese") ~ "asian",
      race == "white" ~ "white",
      race == "other asian or pacific islander" ~ "pacific",
      race == "american indian or alaska native" ~ "american indian or alaska native",
      TRUE ~ "other"
    ),
    age_group = case_when(
      age <=25 ~ "Young",
      age <= 50 ~ "Adult",
      age <= 75 ~ "Senior",
      TRUE ~ "Old"
    ),
    income_group = case_when(
      inctot <= 20000 ~ "Low",
      inctot <= 100000 ~ "Middle",
      inctot <= 200000 ~ "UpperMiddle",
      TRUE ~ "High"
    )
  ) %>% 
  filter(inctot > 0) %>% 
  select(citizen, sex, census_region, race,
         inctot, age, age_group, income_group, stateicp,
         stateicp) %>% 
  na.omit() %>%
  group_by(sex, race, citizen, census_region, stateicp, age_group, income_group) %>%
  group_by(sex, race, citizen, census_region, stateicp, age_group, income_group) %>%
  summarise(n = n(), age = median(age), inctot = median(inctot)) %>% 
  ungroup() %>% 
  mutate(
    weights = n/sum(n)
  )
```

### Post-Stratification

Post-Stratification is necessary in our analysis. That is because, in the survey data, the data collected for each group (we will clarify the meaning of the `group` later) is imbalanced. For example, if we made a survey under the `Trump campaign speech`, most samples would vote for him. However, such group may be a very little proportion under the whole census. If we draw conclusion without performing the post-Stratification, we will conclude that Trump is going to win the election almost 100% which is so wrong. Here we create **seven** cells based on different `sex`, `race`, `citizen`, `region`, `age group`, `income group` (the details of constructing `age group` and `income group` are shown in appendix) and `state`. Each cell represents one group. Here, group means that all people under such group share the same tags (e.g. they all white, male, young, with middle income, etc). 

# Results

As the predicted $\hat{\mu}$ is the log odds, the probability can be achieved as

\[\hat{p} = \frac{e^{\hat{\mu}}}{1 + e^{\hat{\mu}}}\]

The predicted $\hat{y}$ (vote for Trump) is 

\[\hat{y} = \left\{\begin{array}{lcl}
1 & ~~~ & \hat{p} \geq 0.5
\\&&\\
0 && \mbox{otherwise}
\end{array}
\right.
\]

The post-stratification predicted $\hat{Y}^{ps}$

\[\hat{Y^{ps}} =\sum_{h = 1}^H{P_h\hat{Y_h}}\]

where $P_h$ is the proportion estimate by the respective population size $\sum_{h = 1}^HP_h = 1$; $\hat{\bar{Y_h}}$ represents the estimated vote in the given cell. 

Figure 1 is a bar chart of the post-stratification predicted probability for each candidate. We estimate that the proportion of voters in favor of voting for Democratic Party to be 0.811 ($\hat{Y^{ps}}$). This is based off our post-stratification analysis of the proportion of voters in favor of Joe Biden modelled by a `glm` model, which accounted for variable `sex`, `race`, `citizen`, `region`, `age` and `income`.

```{r eistmate, fig.width=4, fig.height=3, echo=FALSE, fig.align='center', message = FALSE, warning = FALSE, error=FALSE}
# model prediction
estimate <- predict(model, newdata = census_data)
# get the prob
prob <- exp(estimate)/(1 + exp(estimate))
prob[prob >= 0.5] <- 1
prob[prob < 0.5] <- 0

predict_vote_trump <- prob
predict_vote_biden <- !prob
bar <- data.frame(
  candidates = c("Trump", "Biden"),
  win_prob = c(sum(predict_vote_trump)/length(prob), sum(predict_vote_biden)/length(prob))
) %>% 
  ggplot() + 
  geom_col(aes(x = candidates, y = win_prob),
           fill = "grey85") + 
  geom_text(
    data.frame(x = c("Biden", "Trump"),
               y = c(0.4, 0.4),
               label = c("81.1%", "18.9%")),
    mapping= aes(x =x, y = y, label = label)
  ) + 
  theme_classic() + 
  labs(
    caption = "Figure 1"
  )
bar
```

Then we need to calculate the adjusted votes grouped by states. Since in US, most state laws establish a `winner-take-all` system, wherein the ticket that wins a plurality of votes wins all of that state's allocated electoral votes. This may lead to an issue, the one who obtain the most votes might not win the election (e.g. In 2000, Al Gore got more votes than George W.Bush, however under such rule, George W.Bush won the election). Figure 2 represents the chance to win for candidates in each state (the detailed table is shown in appendix)

```{r, fig.width=6, fig.height=4, echo=FALSE, message = FALSE, warning = FALSE, error=FALSE}
# build cells
census_data %>% 
  mutate(predict_vote_trump = predict_vote_trump,
         predict_vote_biden = predict_vote_biden) %>% 
  group_by(stateicp) %>% 
   summarise(trump_win = sum(predict_vote_trump)/n(),
             biden_win = sum(predict_vote_biden)/n()) %>% 
  ungroup() %>% 
  mutate(
    take = case_when(
      biden_win > trump_win ~ "Biden", 
      biden_win <= trump_win ~ "Trump", 
    )
  ) -> reduced_data 
reduced_data %>% 
  tidyr::pivot_longer(cols = c(trump_win, biden_win)) %>% 
  ggplot(
    aes(x = as.numeric(as.factor(stateicp)),
        y = value,
        colour = name)
  ) + 
  geom_path() + 
  scale_x_continuous(breaks = 1:51, labels = reduced_data$stateicp) + 
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) + 
  labs(
    x = "",
    y = "probability",
    title = "Chance to win in each state",
    caption = "Figure 2"
  )
```

Joe Biden has a dominated advantage and almost takes all states.

# Summary and Conclusion

In this analysis, we build a generalized linear regression model on all factors. But considering the multi-collinearity, a stepwise BIC process is provided. After variable selection, we use `citizen`, `sex`, `region`, `race`, `total income` and `age` to predict the votes. The summary of the coefficients is

```{r, fig.width=8, fig.height=4, echo=FALSE, message = FALSE, warning = FALSE, error=FALSE}
model <- MASS::stepAIC(glm(vote_trump~., data = survey_data, 
                           family = "binomial"), 
                       trace = FALSE,
                       k = log(dim(survey_data)[1]))
knitr::kable(
  summary(model)$coefficients,
  digits = 2
)
```

Through the summaries, we note that

* White male favor Trump more than others.

* Trump is popular in South. 

* The aged group people are more likely to vote for Trump. 

* Citizens favor Trump more.

In the forecast, post-stratification technique is activated. The main reason is that the survey sample maybe highly imbalanced. To decrease such bias, we assign weights to each group by the respective population size. 

Results show that Joe Biden will win the 2020 election in a great advantage. He almost takes all states and will be elected as the new 46th US president. 

## Weaknesses and Next Step

Apparently, such forecast is not reliable since some states are called red states, such as Alabama, Alaska, Arizona, etc. Those states has very little chance to be blue. Let us discuss the weakness of this analysis in the following two parts, `data` and `model`.

### Data

To predict the election correctly is a very difficult job, especially some candidate has strong personal touch, such as Donald Trump. In 2016 election, almost all predicates point that Hillary Clinton will be in the house, however, due to the 'email scandal' [@salahub2018her] and some other reasons, Trump unexpected won the election. One of the reasons could be that, when researchers conduct a survey, some rural whites, low income people are not given enough considerations. In other word, the sample is highly biased. Most Biden's supporters are high-educated, urban people who are much easier to access the questionnaires. Conversely, most Trump's supporters are in the rural area, the amount of these people is large and it is difficult to include them well in the samples. Even we can use the post-stratification to adjust the rate. However, the bias could still be large. In the next step, a follow-up survey should be constructed with focus on the people living in the rural area. Since such people may not get used to mobile devices or network, we should ask some researchers to do a door-to-door survey. Nevertheless, such work may take very high cost.

### Model

The generalized linear regression is appropriate, however, we do not include any intersections. Variables could not be perfectly independent with each other. Including the intersection terms can help us better capture the structure of this data set. The trade off of such operation would be that the model complexity could increase dramatically, so does the computing time. 

We build **six** cells for post-stratification, the benefit is that the model is more accurate. However, the drawback is that the number of samples in each cell may not be descent.

In the next step, we will try to bring more intersection terms. In addition, we can select the model via variety methods, such AIC, BIC, Mallows's Cp[@mallows2000some], etc. Then, generating multiple versions of a predictor and using these to get an aggregated predictor [@breiman1996bagging]. The benefit of doing so is that the results do not rely on one specific model.

\pagebreak

# Appendix

### Survey Data Modification

Some variables need to be modified. For example:

* Factor `education` contains 11 levels indicating that we have to create 10 dummy variables to suit the case. It is a waste of the degree of freedom. Thus, we merge the 11 levels to 3, which are "high school", "bachelor" and "graduate" (identical operations will be applied on the census data as well). 

* Factor `household income` is a categorical variable containing 25 different levels. We would like to treat it as a numerical variable. The greatest benefit is that we can save the degree of freedoms (avoid 24 more dummy variables). To each observation, we would like to random generate the salary in such level by empirical rule (e.g. the household income of person `A` is "\$85,000 to \$89,999", the random generated income would be `rnorm(1, mean = 87500, sd = 2500/3)`)

### Census Data Modification

Note that age and income are numerical variable (in census data set). We will classify them in several levels so that they can be treated as a cell in good form. In our case we set that

* Age: people less than 25 are marked as "Young"; people who are between 25 to 50 are marked as "Adult"; people who are between 50 to 75 are marked as "Senior" and people who are over 75 are marked as "Old".

* Total Income: the total income is less than $\$20,000$ per year are marked as "Low"; the total income is between $\$20,000$ to $\$100,000$ per year are marked as "Middle"; the total income is between $\$100,000$ to $\$200,000$ are marked as "Upper Middle" and the total income is over $\$200,000$ are marked as "High".

### Prediction table

```{r, fig.width=6, fig.height=4, echo=FALSE, fig.align='center', message = FALSE, warning = FALSE, error=FALSE}
reduced_data %>% 
  knitr::kable(digits = 3)
```

\pagebreak

# References








